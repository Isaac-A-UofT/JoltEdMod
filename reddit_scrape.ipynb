{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f16c7a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome, dear social scientists, psychologists, and academics from the humanities! Today, we embark on an exciting journey to explore the world of online discussions through the lens of Reddit. Our goal is to use the power of Python and the PRAW package to scrape Reddit data, which can be a treasure trove of insights for our research.\n",
    "\n",
    "# Reddit as an Online Agora\n",
    "\n",
    "Imagine Reddit as a massive digital agora - a central public space in ancient Greek city-states, where people gathered to exchange ideas, discuss politics, philosophy, and various aspects of life. In this modern-day agora, there are countless conversations happening at any given moment, with people sharing their opinions, experiences, and stories, making it an invaluable resource for understanding human behavior.\n",
    "\n",
    "# Entering the Agora with Python and PRAW\n",
    "\n",
    "As researchers, we want to observe and analyze these conversations, but the sheer volume of data can be overwhelming. This is where Python and the PRAW package come in as our friendly guides to help us navigate this bustling space.\n",
    "\n",
    "Think of Python as a versatile and powerful companion that can help us perform various tasks, from the simplest to the most complex. PRAW, on the other hand, is a specialized tool designed to interact with Reddit's API, allowing us to access and collect data from the platform in a structured and efficient way.\n",
    "\n",
    "# The Journey to Collect Data\n",
    "\n",
    "Our journey to collect data from Reddit using Python and PRAW can be conceptualized in four main steps:\n",
    "\n",
    "1. **Gaining Access**: First, we must request access to the agora (Reddit), just as one would need permission to enter an ancient city-state. In this case, we obtain the necessary credentials (such as a client ID and secret) by registering a new application on the Reddit website.\n",
    "\n",
    "2. **Establishing Connection**: Once we have the credentials, we can use Python and PRAW to establish a connection to Reddit's API. This is akin to opening the gates to the agora, allowing us to interact with the platform and gather the information we seek.\n",
    "\n",
    "3. **Exploring Subreddits and Posts**: Now that we have access, we can start exploring different areas of the agora. In Reddit terms, these areas are called \"subreddits,\" each dedicated to a specific topic. For example, there are subreddits for politics, philosophy, psychology, and many other subjects. We can use Python and PRAW to navigate these subreddits, select the ones relevant to our research, and extract the posts (conversations) happening within them.\n",
    "\n",
    "4. **Diving into Comments**: Once we have collected the posts, we can delve deeper into the conversations by extracting the comments (individual responses) associated with each post. This allows us to analyze the opinions, sentiments, and interactions of the Reddit users at a granular level.\n",
    "\n",
    "# Analyzing the Harvest\n",
    "\n",
    "After collecting the data, we can use various Python libraries and tools to clean, process, and analyze the information. Depending on our research focus, we may choose to perform sentiment analysis, topic modeling, social network analysis, or any other method that helps us uncover the insights we seek.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Scraping Reddit data with Python and PRAW is like embarking on an exciting expedition to explore and understand the dynamics of the online agora. With the right tools and approach, we can harness the power of this vast amount of data to enrich our research and gain a deeper understanding of human behavior in the digital age.\n",
    "\n",
    "Happy exploring, dear researchers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff5692",
   "metadata": {},
   "source": [
    "# Scraping Reddit with Python PRAW Package for Social Scientists, Psychologists, and Academics from the Humanities\n",
    "\n",
    "In this tutorial, we will learn how to scrape data from Reddit using the Python PRAW (Python Reddit API Wrapper) package. This tutorial is designed for social scientists, psychologists, and academics from the humanities who may not have a strong background in programming. We will break down each step to make it easy to understand and follow along.\n",
    "\n",
    "Reddit is a treasure trove of information, with various subreddits (topic-specific forums) catering to almost every imaginable interest. By scraping Reddit, you can gather valuable data for your research projects and gain insights into various user behaviors and trends.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we begin, please ensure you have the following installed on your computer:\n",
    "\n",
    "1. Python: If you don't already have Python installed, download it from the official website: https://www.python.org/downloads/\n",
    "2. PRAW: After installing Python, open your command prompt or terminal and install the PRAW package using the following command: `pip install praw`\n",
    "\n",
    "With the prerequisites out of the way, let's dive into the process of scraping Reddit with PRAW!\n",
    "\n",
    "## Setting Up a Reddit App\n",
    "\n",
    "To use PRAW, we first need to create a Reddit App that will allow us to access the Reddit API. Follow these steps:\n",
    "\n",
    "1. Log in to your Reddit account, or create a new one at https://www.reddit.com/register/.\n",
    "2. Go to https://www.reddit.com/prefs/apps and click on the \"Create App\" or \"Create Another App\" button at the bottom of the page.\n",
    "3. Fill in the required fields:\n",
    "   - **name**: Choose a name for your app, e.g., \"My Reddit Scraper\".\n",
    "   - **App type**: Select \"script\".\n",
    "   - **description**: Write a brief description of your app (optional).\n",
    "   - **about url**: Leave this field blank.\n",
    "   - **redirect uri**: Enter \"http://localhost:8080\".\n",
    "   - **permissions**: Leave this as \"read\".\n",
    "4. Click \"Create app\" to finish setting up your Reddit App.\n",
    "\n",
    "After creating the app, you will see a \"client ID\" and \"client secret\" on the app details page. We will use these values in our Python script to authenticate our Reddit scraper.\n",
    "\n",
    "## Python Script to Scrape Reddit\n",
    "\n",
    "Now that we have our Reddit App set up, let's create a Python script using the PRAW package to scrape Reddit.\n",
    "\n",
    "```python\n",
    "# Import the required libraries\n",
    "import praw\n",
    "\n",
    "# Set up the Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id='YOUR_CLIENT_ID',\n",
    "    client_secret='YOUR_CLIENT_SECRET',\n",
    "    user_agent='YOUR_APP_NAME'\n",
    ")\n",
    "\n",
    "# Choose a subreddit to scrape\n",
    "subreddit = reddit.subreddit('AskReddit')\n",
    "\n",
    "# Scrape the top 10 posts in the subreddit\n",
    "for post in subreddit.top(limit=10):\n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"Author: {post.author}\")\n",
    "    print(f\"Score: {post.score}\")\n",
    "    print(f\"URL: {post.url}\")\n",
    "    print(\"-\" * 80)\n",
    "```\n",
    "\n",
    "Replace `'YOUR_CLIENT_ID'`, `'YOUR_CLIENT_SECRET'`, and `'YOUR_APP_NAME'` with the actual values from your Reddit App.\n",
    "\n",
    "In this script, we first import the `praw` library and set up a Reddit API client using our Reddit App's credentials. Then, we choose a subreddit to scrape (in this case, `AskReddit`). Finally, we iterate through the top 10 posts in the subreddit and print their title, author, score, and URL.\n",
    "\n",
    "Feel free to modify the script to suit your specific needs, such as changing the subreddit or the number of posts to scrape.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, we learned how to set up a Reddit App, create a Python script using the PRAW package, and scrape data from a subreddit. This powerful tool can be a valuable addition to your research toolkit, allowing you to gather insights from a diverse range of topics and user interactions.\n",
    "\n",
    "Happy scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df4460",
   "metadata": {},
   "source": [
    "Title: Analyzing Sentiments of Reddit Users in a Specific Community\n",
    "\n",
    "**Problem**\n",
    "\n",
    "As a social scientist or psychologist, you are interested in understanding the sentiments of users in a specific subreddit community. You want to collect a dataset containing the titles of the top 100 posts, along with their respective authors, post scores, and the number of comments. You will use the PRAW (Python Reddit API Wrapper) package to scrape Reddit and export the data to a CSV file.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "1. Install the PRAW package, if you haven't already. You can do this using pip:\n",
    "\n",
    "```\n",
    "pip install praw\n",
    "```\n",
    "\n",
    "2. Set up a Reddit App to obtain the necessary credentials (client ID and secret) by following these steps:\n",
    "   - Visit https://www.reddit.com/prefs/apps\n",
    "   - Click \"Create App\" or \"Create Another App\"\n",
    "   - Fill in the required information:\n",
    "     * Choose \"script\" as the app type\n",
    "     * Set a name, e.g., \"Sentiment Analyzer\"\n",
    "     * Set a redirect URI, e.g., \"http://localhost:8080\"\n",
    "     * Add a description (optional)\n",
    "   - Click \"Create App\" and note the client ID (below the app name) and client secret\n",
    "\n",
    "3. Write a Python script that:\n",
    "   - Imports the necessary libraries (e.g., praw, csv)\n",
    "   - Authenticates with Reddit using the PRAW package and the credentials obtained in step 2\n",
    "   - Scrapes the top 100 posts from a specified subreddit (e.g., r/AskReddit)\n",
    "   - Extracts the post title, author, score, and number of comments for each post\n",
    "   - Exports the data to a CSV file\n",
    "\n",
    "4. Analyze the data to find patterns, trends, or other insights.\n",
    "\n",
    "**Tips**\n",
    "\n",
    "- When initializing the Reddit instance in PRAW, use the following syntax:\n",
    "\n",
    "```python\n",
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='your_client_id',\n",
    "                     client_secret='your_client_secret',\n",
    "                     user_agent='sentiment-analyzer')\n",
    "```\n",
    "\n",
    "- To get the top 100 posts in a specific subreddit, use:\n",
    "\n",
    "```python\n",
    "subreddit = reddit.subreddit('AskReddit')\n",
    "top_posts = subreddit.top(limit=100)\n",
    "```\n",
    "\n",
    "- To extract the post title, author, score, and number of comments, you can loop through the posts and access their attributes:\n",
    "\n",
    "```python\n",
    "for post in top_posts:\n",
    "    title = post.title\n",
    "    author = post.author\n",
    "    score = post.score\n",
    "    num_comments = post.num_comments\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302f0488-e669-4e28-ad08-b57d3c88fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods correctly.\n",
    "\n",
    "**Code with Empty Methods**\n",
    "\n",
    "Now that you have an understanding of the requirements and tips, let's create empty methods with comments for what they should do. Later, you can implement these methods to complete the task.\n",
    "\n",
    "```python\n",
    "import praw\n",
    "import csv\n",
    "\n",
    "def authenticate_reddit():\n",
    "    # This method should authenticate with Reddit using PRAW and return a Reddit instance\n",
    "    pass\n",
    "\n",
    "def get_top_posts(reddit_instance, subreddit_name, limit=100):\n",
    "    # This method should return the top 'limit' posts from the specified subreddit\n",
    "    pass\n",
    "\n",
    "def extract_post_data(posts):\n",
    "    # This method should extract the title, author, score, and number of comments for each post and return a list of dictionaries\n",
    "    pass\n",
    "\n",
    "def export_to_csv(post_data, file_name):\n",
    "    # This method should export the post data to a CSV file with the specified file_name\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Assertion Tests**\n",
    "\n",
    "Here are three assertion tests you can use to test your implementation:\n",
    "\n",
    "1. Test the `authenticate_reddit()` method:\n",
    "\n",
    "```python\n",
    "reddit_instance = authenticate_reddit()\n",
    "assert isinstance(reddit_instance, praw.Reddit), \"Authentication failed. Check your credentials.\"\n",
    "```\n",
    "\n",
    "2. Test the `get_top_posts()` method:\n",
    "\n",
    "```python\n",
    "top_posts = get_top_posts(reddit_instance, 'AskReddit', limit=10)\n",
    "assert len(top_posts) == 10, \"The number of top posts retrieved is incorrect.\"\n",
    "```\n",
    "\n",
    "3. Test the `extract_post_data()` method:\n",
    "\n",
    "```python\n",
    "post_data = extract_post_data(top_posts)\n",
    "assert isinstance(post_data, list) and len(post_data) == 10, \"Post data extraction is incorrect. Check the method implementation.\"\n",
    "for post in post_data:\n",
    "    assert all(key in post for key in ['title', 'author', 'score', 'num_comments']), \"Post data is missing one or more required attributes.\"\n",
    "```\n",
    "\n",
    "To complete the task, implement the methods in the provided code and run the assertion tests to ensure everything is working correctly. Once you've successfully collected the data, you can proceed to analyze it for patterns, trends, or other insights."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
