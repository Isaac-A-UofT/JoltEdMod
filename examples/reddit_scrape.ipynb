{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1498463",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Hello, dear social scientists, psychologists, and academics from the humanities! Today, we will embark on an exciting journey through the world of Reddit, one of the largest and most diverse online communities. Our goal is to learn how to harness the power of Python and a package called PRAW to explore the vast landscape of human interactions on this platform. Just as anthropologists study different cultures and historians examine documents to understand the past, we will use PRAW to \"scrape\" Reddit and gather valuable data for our research.\n",
    "\n",
    "# The Metaphorical Landscape\n",
    "\n",
    "Imagine Reddit as a gigantic library, where each book represents a subreddit, a separate community devoted to a specific topic. Each book is filled with numerous stories (posts), and within these stories, people engage in discussions, exchange ideas, and share their thoughts (comments). As researchers, we are interested in studying these interactions, identifying patterns, and drawing valuable insights.\n",
    "\n",
    "Python, as our \"guide\" in this library, will help us navigate through the vast collection of books and select the ones we're interested in. PRAW, on the other hand, is our \"toolkit,\" a collection of gadgets and gizmos that enable us to interact with the library in a more efficient and organized manner.\n",
    "\n",
    "# The Adventure Begins\n",
    "\n",
    "To begin our adventure, we first need to equip ourselves with the PRAW toolkit. We will install it and set it up to be able to access the Reddit library. Once we have our toolkit ready, we can set foot in the library and start exploring.\n",
    "\n",
    "Equipped with PRAW, we can easily locate the books (subreddits) we want to study. We can even find the most popular or the newest books, depending on our research interests. Once we have picked our books, we can dive into the stories (posts) inside them. We can read the stories, make notes, and even observe how many people have read and appreciated them.\n",
    "\n",
    "As we progress through our journey, we will also encounter the heart and soul of Reddit - the discussions (comments). PRAW enables us to not only read these discussions but also look closely at the people participating in them. We can learn about their backgrounds, interests, and how they contribute to the community.\n",
    "\n",
    "During our exploration, we will come across a wealth of data that can be used to answer various research questions. Social scientists may use this data to study group dynamics, psychologists to understand human behavior, and humanities scholars to analyze cultural trends and discourses.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this metaphorical journey, we have only scratched the surface of what is possible with Python and the PRAW package. As we continue to expand our skillset and explore the depths of Reddit, the potential for research is limited only by our imagination. So, equip your PRAW toolkit, and let the adventure begin!\n",
    "\n",
    "In the next part of this series, we will dive into the practical aspects of setting up PRAW, accessing Reddit data, and extracting useful information for our research. Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf6c100",
   "metadata": {},
   "source": [
    "# Scraping Reddit with the Python PRAW Package\n",
    "\n",
    "Hello social scientists, psychologists, and academics from the humanities! Today, we will explore how to gather data from Reddit, a popular social media platform, using the Python Reddit API Wrapper (PRAW) package. This tutorial will guide you through the process of setting up the PRAW package, authenticating with Reddit, and extracting data from various subreddits and posts. By the end of this tutorial, you will be able to use Python and PRAW to gather data from Reddit for your research projects.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we start, you'll need to have Python installed on your computer. If you don't have it already, you can download it from the [official Python website](https://www.python.org/downloads/). You'll also need to have a Reddit account, as we'll be using Reddit's API to access the data.\n",
    "\n",
    "## Step 1: Install PRAW\n",
    "\n",
    "First, let's install the PRAW package using pip. Open your terminal or command prompt and run the following command:\n",
    "\n",
    "```\n",
    "pip install praw\n",
    "```\n",
    "\n",
    "This command will install PRAW and its dependencies on your computer.\n",
    "\n",
    "## Step 2: Create a Reddit App\n",
    "\n",
    "To use the Reddit API, you need to create an application on the Reddit website. Follow these steps:\n",
    "\n",
    "1. Log in to your Reddit account.\n",
    "2. Go to [https://www.reddit.com/prefs/apps](https://www.reddit.com/prefs/apps).\n",
    "3. Scroll down to the \"Developed Applications\" section and click \"Create App\" or \"Create Another App.\"\n",
    "4. Fill out the form with the following details:\n",
    "    * \"name\": Choose a name for your app (e.g., \"MyRedditScraper\").\n",
    "    * \"App type\": Select \"script.\"\n",
    "    * \"description\": Write a brief description of your app (optional).\n",
    "    * \"about url\": Leave this field empty.\n",
    "    * \"redirect uri\": Enter \"http://localhost:8080\" (without quotes).\n",
    "    * \"permissions\": Leave the default setting.\n",
    "5. Click \"Create app.\"\n",
    "\n",
    "Once your app is created, you will see a \"client ID\" and \"client secret\" on the app details page. We will use these credentials to authenticate with the Reddit API.\n",
    "\n",
    "## Step 3: Set up PRAW\n",
    "\n",
    "Now that we have our Reddit app and PRAW installed, let's set up PRAW to access the Reddit API. Create a new Python script or Jupyter Notebook and import the PRAW package:\n",
    "\n",
    "```python\n",
    "import praw\n",
    "```\n",
    "\n",
    "Next, create a Reddit instance using your app credentials:\n",
    "\n",
    "```python\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"your_client_id\",\n",
    "    client_secret=\"your_client_secret\",\n",
    "    user_agent=\"your_user_agent\",\n",
    "    username=\"your_reddit_username\",\n",
    "    password=\"your_reddit_password\",\n",
    ")\n",
    "```\n",
    "\n",
    "Replace the placeholders with your actual credentials:\n",
    "\n",
    "- `your_client_id`: The client ID from your Reddit app.\n",
    "- `your_client_secret`: The client secret from your Reddit app.\n",
    "- `your_user_agent`: A unique and descriptive user agent string (e.g., \"my_reddit_scraper v1.0\").\n",
    "- `your_reddit_username`: Your Reddit username.\n",
    "- `your_reddit_password`: Your Reddit password.\n",
    "\n",
    "## Step 4: Extract Data from Reddit\n",
    "\n",
    "With PRAW set up, we can now access various Reddit entities such as subreddits and posts. Let's start by accessing a specific subreddit:\n",
    "\n",
    "```python\n",
    "subreddit = reddit.subreddit(\"AskSocialScience\")\n",
    "```\n",
    "\n",
    "Replace \"AskSocialScience\" with the name of the subreddit you want to access. \n",
    "\n",
    "Now, let's extract the top 10 posts from the subreddit:\n",
    "\n",
    "```python\n",
    "top_posts = subreddit.top(limit=10)\n",
    "```\n",
    "\n",
    "This code will retrieve the top 10 posts from the specified subreddit. You can adjust the `limit` parameter to fetch more or fewer posts.\n",
    "\n",
    "Finally, let's print the titles and scores of the top 10 posts:\n",
    "\n",
    "```python\n",
    "for post in top_posts:\n",
    "    print(f\"Title: {post.title}, Score: {post.score}\")\n",
    "```\n",
    "\n",
    "This code will iterate through the top 10 posts and print their titles and scores.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, we learned how to set up PRAW, authenticate with the Reddit API, and extract data from Reddit for research purposes. By using PRAW, you can easily gather data from Reddit to support your research in social sciences, psychology, and humanities.\n",
    "\n",
    "This tutorial provides just a basic introduction to PRAW, which offers many more features for accessing and interacting with Reddit data. To learn more about PRAW, please refer to the [official PRAW documentation](https://praw.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da458cf9",
   "metadata": {},
   "source": [
    "Title: Analyzing Reddit Discussions in a Specific Subreddit using the Python PRAW Package\n",
    "\n",
    "Objective: In this programming problem, we will explore how to use Python and the PRAW (Python Reddit API Wrapper) package to scrape discussion data from a specific subreddit. This is suitable for social scientists, psychologists, and academics from the humanities who are interested in studying online communities, language patterns, or any other feature of Reddit discussions.\n",
    "\n",
    "Problem Description:\n",
    "\n",
    "Your task is to write a Python script that:\n",
    "\n",
    "1. Authenticates with the Reddit API using PRAW.\n",
    "2. Extracts the top 50 posts from a given subreddit of your choice (e.g., /r/AskSocialScience).\n",
    "3. For each post, retrieves the title, author, and top 5 comments (including the comment author and comment text).\n",
    "4. Exports the data to a CSV file for further analysis.\n",
    "\n",
    "Here's a list of steps to guide your implementation:\n",
    "\n",
    "1. Install the PRAW package:\n",
    "   If you haven't installed PRAW, you can do so by running the following command in your terminal or command prompt:\n",
    "   \n",
    "   ```\n",
    "   pip install praw\n",
    "   ```\n",
    "\n",
    "2. Register for a Reddit account and create a new application to obtain your API credentials (client ID and client secret). Follow the instructions in this guide: https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example#first-steps\n",
    "\n",
    "3. Set up your Python script with the following imports and configuration:\n",
    "\n",
    "   ```python\n",
    "   import praw\n",
    "   import csv\n",
    "\n",
    "   reddit = praw.Reddit(\n",
    "       client_id=\"your_client_id\",\n",
    "       client_secret=\"your_client_secret\",\n",
    "       user_agent=\"your_user_agent\",\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. Write a function to retrieve the top 50 posts from a given subreddit:\n",
    "\n",
    "   ```python\n",
    "   def get_top_posts(subreddit_name, num_posts=50):\n",
    "       # Your code here\n",
    "   ```\n",
    "\n",
    "5. Write a function to extract the title, author, and top 5 comments from a post:\n",
    "\n",
    "   ```python\n",
    "   def extract_post_data(post, num_comments=5):\n",
    "       # Your code here\n",
    "   ```\n",
    "\n",
    "6. Iterate over the top 50 posts, extract the data using the `extract_post_data()` function, and store the results in a list.\n",
    "\n",
    "7. Write a function to export the data to a CSV file:\n",
    "\n",
    "   ```python\n",
    "   def export_to_csv(data, filename=\"reddit_data.csv\"):\n",
    "       # Your code here\n",
    "   ```\n",
    "\n",
    "8. Call the `export_to_csv()` function to save the data to a CSV file.\n",
    "\n",
    "Once you have completed the task, you should have a Python script that extracts data from a specific subreddit and exports it to a CSV file. You can then use this data to perform various analyses, such as sentiment analysis or topic modeling, depending on your research interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a1630-6442-4570-a571-60cbf3e5baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods correctly:\n",
    "\n",
    "```python\n",
    "def test_get_top_posts():\n",
    "    # Test if the get_top_posts function returns the correct number of posts\n",
    "    # Your code here\n",
    "\n",
    "def test_extract_post_data():\n",
    "    # Test if the extract_post_data function returns the expected data for a given post\n",
    "    # Your code here\n",
    "\n",
    "def test_export_to_csv():\n",
    "    # Test if the export_to_csv function creates a valid CSV file with the correct data\n",
    "    # Your code here\n",
    "```\n",
    "\n",
    "To use these tests, you'll need to implement the functions `get_top_posts`, `extract_post_data`, and `export_to_csv`. Then, you can call these test functions in the main part of your script to check if your implementation is working correctly.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    test_get_top_posts()\n",
    "    test_extract_post_data()\n",
    "    test_export_to_csv()\n",
    "    print(\"All tests passed\")\n",
    "```\n",
    "\n",
    "If any of the tests fail, you will need to review your implementation and make any necessary adjustments. Once all tests pass, you can be confident that your script is working correctly and proceed to use it for your research purposes."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
