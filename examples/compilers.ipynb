{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2997285f",
   "metadata": {},
   "source": [
    "# Compiler Design: Building a Lexical Analyzer and Syntax Analyzer (Parser) for C in Python\n",
    "\n",
    "Welcome, dear students, to this exciting chapter on compiler design! As future computer scientists, you will uncover the mysteries of how your code is transformed into something your computer can understand. Today, we'll take a journey through the process of creating a lexical analyzer and a syntax analyzer (parser) for the C programming language using Python. Let's begin by exploring the world of compilers.\n",
    "\n",
    "## The Compiler Metaphor\n",
    "\n",
    "Imagine you are an expert linguist who can understand and translate multiple languages. Your task is to ensure that people from different countries can communicate with one another effectively. To do this, you must first listen to their words (the **source code**), decipher the meaning behind them (**lexical analysis**), and then organize the phrases into coherent sentences (**syntax analysis**). This is much like how a compiler works, translating high-level programming languages like C into something a computer can execute.\n",
    "\n",
    "### Lexical Analysis: The Word Detective\n",
    "\n",
    "As a linguist, your first challenge is to examine the words and identify their roles. This process, called **lexical analysis**, is like breaking down a sentence into individual words, punctuation marks, and whitespace. In our compiler metaphor, the lexical analyzer is the detective who investigates the source code and classifies each element.\n",
    "\n",
    "For our C language compiler, the lexical analyzer will identify keywords, identifiers, operators, and other language constructs. Take, for example, the following C code:\n",
    "\n",
    "```c\n",
    "int main() {\n",
    "    int a = 5;\n",
    "    return a * a;\n",
    "}\n",
    "```\n",
    "\n",
    "The lexical analyzer in this case must recognize `int`, `main`, `(`, `)`, `{`, `}`, `a`, `5`, `=`, `*`, `;`, and `return` as separate **tokens**.\n",
    "\n",
    "### Syntax Analysis: The Sentence Builder\n",
    "\n",
    "Once you've identified each word, the next step is to arrange them in a way that makes sense. This is called **syntax analysis** or **parsing**. As a linguist, you must understand the grammar and sentence structure of the languages you're working with to create meaningful translations. In programming languages, this involves understanding the rules that dictate how tokens may be combined.\n",
    "\n",
    "Continuing our example, the syntax analyzer will need to recognize that `int main() { ... }` is a valid function definition, and `int a = 5;` is an appropriate variable declaration and assignment. The parser ensures that the code follows the rules of the C language, and constructs an **Abstract Syntax Tree (AST)** which represents the code's structure.\n",
    "\n",
    "## Building the Lexical Analyzer and Syntax Analyzer for C in Python\n",
    "\n",
    "Now that you have an understanding of the metaphor, let's discuss how you, as a computer scientist, can develop a lexical analyzer and syntax analyzer for the C language using Python.\n",
    "\n",
    "### Lexical Analyzer in Python\n",
    "\n",
    "To design a lexical analyzer, you can use Python's powerful regular expression library, `re`. This allows you to define patterns for each token, such as keywords, operators, and identifiers. You will then use these patterns to tokenize the input source code.\n",
    "\n",
    "### Syntax Analyzer (Parser) in Python\n",
    "\n",
    "For syntax analysis, you can use a library like PLY (Python Lex-Yacc) which provides lex and yacc parsing tools for Python. You'll define grammar rules for your C language in the form of production rules, which dictate how tokens can be combined to form valid constructs. The parser will then use these rules to generate the AST.\n",
    "\n",
    "As you progress through this chapter, we will dive deeper into the implementation details, providing you with the necessary knowledge to build your very own lexical analyzer and syntax analyzer for the C language using Python. This will not only expand your understanding of compiler design but also enhance your programming skills as a whole. So, let's embark on this wonderful journey together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e31db",
   "metadata": {},
   "source": [
    "# Compiler Design: Building a Lexical Analyzer and Syntax Analyzer (Parser) for C in Python\n",
    "\n",
    "Welcome, dear fourth-year computer science students! In this chapter, we'll explore the fascinating world of compiler design by building a lexical analyzer and a syntax analyzer (parser) for the C programming language, using Python. By the end of this chapter, you'll have a solid understanding of how compilers work, and you'll be able to apply these concepts to other programming languages.\n",
    "\n",
    "## Overview of Compiler Design\n",
    "\n",
    "A compiler is a program that translates source code written in a high-level programming language (like C) into a lower-level language (such as assembly language or machine code) that can be executed by a computer. The process of compilation can be divided into several stages:\n",
    "\n",
    "1. Lexical Analysis\n",
    "2. Syntax Analysis (Parsing)\n",
    "3. Semantic Analysis\n",
    "4. Intermediate Code Generation\n",
    "5. Code Optimization\n",
    "6. Code Generation\n",
    "\n",
    "In this chapter, we'll focus on the first two stages: Lexical Analysis and Syntax Analysis. Let's dive in!\n",
    "\n",
    "## Lexical Analysis\n",
    "\n",
    "The lexical analyzer (also known as the scanner or tokenizer) is responsible for breaking the input source code into a sequence of tokens. Tokens are the basic building blocks of a programming language, such as keywords, identifiers, literals, punctuation marks, and operators.\n",
    "\n",
    "### Building a Lexical Analyzer for C in Python\n",
    "\n",
    "To build a lexical analyzer, we'll use Python's `re` (regular expressions) library to define patterns for each token type. We will then use these patterns to match and tokenize the input source code.\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "# Token types and their corresponding regular expressions\n",
    "TOKEN_TYPES = [\n",
    "    ('KEYWORD', r'\\b(?:if|else|while|for|return|int|float|double|char|void)\\b'),\n",
    "    ('IDENTIFIER', r'\\b[a-zA-Z_][a-zA-Z_0-9]*\\b'),\n",
    "    ('NUMBER', r'\\b\\d+(\\.\\d*)?|\\.\\d+\\b'),\n",
    "    ('OPERATOR', r'[+\\-*/%<>=!]=?'),\n",
    "    ('PUNCTUATION', r'[;()[\\]{},.]'),\n",
    "    ('WHITESPACE', r'\\s+'),\n",
    "    ('COMMENT', r'//.*\\n?|/\\*[\\s\\S]*?\\*/'),\n",
    "]\n",
    "\n",
    "def tokenize(source_code):\n",
    "    tokens = []\n",
    "    index = 0\n",
    "\n",
    "    while index < len(source_code):\n",
    "        for token_type, pattern in TOKEN_TYPES:\n",
    "            match = re.match(pattern, source_code[index:])\n",
    "            if match:\n",
    "                if token_type not in ['WHITESPACE', 'COMMENT']:\n",
    "                    tokens.append((token_type, match.group(0)))\n",
    "                index += len(match.group(0))\n",
    "                break\n",
    "        else:\n",
    "            raise SyntaxError(f'Unexpected character at index {index}: {source_code[index]}')\n",
    "\n",
    "    return tokens\n",
    "```\n",
    "\n",
    "Now that we have our lexical analyzer, let's test it with a simple C code snippet:\n",
    "\n",
    "```python\n",
    "source_code = \"\"\"\n",
    "int main() {\n",
    "    int a = 5;\n",
    "    int b = a + 3;\n",
    "    return b;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "tokens = tokenize(source_code)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "This will output the following sequence of tokens:\n",
    "\n",
    "```\n",
    "[('KEYWORD', 'int'), ('IDENTIFIER', 'main'), ('PUNCTUATION', '('), ('PUNCTUATION', ')'), ...]\n",
    "```\n",
    "\n",
    "## Syntax Analysis (Parsing)\n",
    "\n",
    "The syntax analyzer (or parser) is responsible for checking the sequence of tokens produced by the lexical analyzer to ensure that it conforms to the grammar rules of the programming language. The parser also generates an abstract syntax tree (AST) that represents the structure of the input source code.\n",
    "\n",
    "### Building a Syntax Analyzer for C in Python\n",
    "\n",
    "To build a syntax analyzer for C, we'll use a top-down parsing technique called recursive descent parsing. This involves writing a set of recursive functions, each of which is responsible for parsing a specific grammar rule.\n",
    "\n",
    "We'll start by defining a simple grammar for a subset of the C language. This grammar will be used to guide our parser implementation:\n",
    "\n",
    "```\n",
    "program        -> function\n",
    "function       -> type identifier '(' ')' block\n",
    "block          -> '{' declaration* statement* '}'\n",
    "declaration    -> type identifier ';'\n",
    "statement      -> expression_statement | return_statement\n",
    "expression_statement -> expression ';' | ';'\n",
    "return_statement -> 'return' expression ';'\n",
    "expression     -> assignment\n",
    "assignment     -> identifier '=' expression | additive\n",
    "additive       -> multiplicative (('+' | '-') multiplicative)*\n",
    "multiplicative -> primary (('*' | '/') primary)*\n",
    "primary        -> identifier | number | '(' expression ')'\n",
    "type           -> 'int' | 'float' | 'double' | 'char' | 'void'\n",
    "```\n",
    "\n",
    "Using this grammar, we can now implement our recursive descent parser:\n",
    "\n",
    "```python\n",
    "class Parser:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.index = 0\n",
    "\n",
    "    def parse(self):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d99ff",
   "metadata": {},
   "source": [
    "Title: Compiler Design - Building a Lexical Analyzer and Syntax Analyzer for a Simplified C Language in Python\n",
    "\n",
    "Problem Statement:\n",
    "\n",
    "In this problem, you will design and implement a lexical analyzer (lexer) and a syntax analyzer (parser) for a simplified version of the C language using Python. The simplified C language will support the following constructs:\n",
    "\n",
    "1. Data types: int and float\n",
    "2. Variables: alphanumeric strings starting with a letter or underscore\n",
    "3. Assignment statements: <variable> = <expression>;\n",
    "4. Arithmetic operations: addition, subtraction, multiplication, and division\n",
    "5. Parentheses for grouping expressions\n",
    "6. Whitespace: spaces and tabs should be ignored by the lexer\n",
    "\n",
    "For example, the following code snippet should be supported by your lexer and parser:\n",
    "\n",
    "int x = 3;\n",
    "int y = 5;\n",
    "float z = (x + 2 * y) / 3.0;\n",
    "\n",
    "You should implement the lexer and parser in two separate classes:\n",
    "\n",
    "1. Lexer: This class should tokenize the input string into meaningful tokens, such as keywords (int, float), identifiers (variables), operators (+, -, *, /), etc. You can define a class method called \"tokenize\" that takes the input string and returns a list of tokens.\n",
    "\n",
    "2. Parser: This class should take the list of tokens from the lexer and produce an Abstract Syntax Tree (AST) based on the grammar rules defined for the simplified C language. You can define a class method called \"parse\" that takes the list of tokens and returns the AST.\n",
    "\n",
    "Here is a high-level overview of the grammar rules for the simplified C language:\n",
    "\n",
    "```\n",
    "program: statement_list\n",
    "statement_list: (statement)*\n",
    "statement: variable_declaration | assignment_statement\n",
    "variable_declaration: data_type IDENTIFIER ';'\n",
    "data_type: \"int\" | \"float\"\n",
    "assignment_statement: IDENTIFIER \"=\" expression ';'\n",
    "expression: term ((\"+\") | (\"-\")) term)*\n",
    "term: factor ((\"*\") | (\"/\")) factor)*\n",
    "factor: \"(\" expression \")\" | NUMBER | IDENTIFIER\n",
    "```\n",
    "\n",
    "You can use any parsing technique (recursive descent, LL(1), LR(1), etc.) to implement the parser. The output AST should be represented as a nested Python data structure (e.g., lists, dictionaries, or custom classes).\n",
    "\n",
    "Your final submission should include:\n",
    "\n",
    "1. The Lexer class and its tokenize method\n",
    "2. The Parser class and its parse method\n",
    "3. Test cases demonstrating the functionality of your lexer and parser with various inputs, including edge cases.\n",
    "\n",
    "Remember to follow good software engineering practices, such as writing clean, modular code, and including comments to explain your design choices.\n",
    "\n",
    "Good luck, and have fun designing your compiler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725e9ab-d646-4b25-a85b-5219c5d993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution correctly.\n",
    "\n",
    "```python\n",
    "class Lexer:\n",
    "    def tokenize(self, input_string):\n",
    "        \"\"\"\n",
    "        This method should take the input string and tokenize it into meaningful tokens, such as keywords (int, float),\n",
    "        identifiers (variables), operators (+, -, *, /), etc.\n",
    "        \n",
    "        :param input_string: A string representing the simplified C code.\n",
    "        :return: A list of tokens.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Parser:\n",
    "    def parse(self, tokens):\n",
    "        \"\"\"\n",
    "        This method should take the list of tokens from the lexer and produce an Abstract Syntax Tree (AST) based on\n",
    "        the grammar rules defined for the simplified C language.\n",
    "\n",
    "        :param tokens: A list of tokens produced by the Lexer.tokenize method.\n",
    "        :return: The AST represented as a nested Python data structure (e.g., lists, dictionaries, or custom classes).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# Test cases for the Lexer and Parser classes\n",
    "def test_compiler():\n",
    "    lexer = Lexer()\n",
    "    parser = Parser()\n",
    "\n",
    "    # Test case 1\n",
    "    input_code = \"int x = 3;\\nint y = 5;\\nfloat z = (x + 2 * y) / 3.0;\"\n",
    "    tokens = lexer.tokenize(input_code)\n",
    "    ast = parser.parse(tokens)\n",
    "    assert ast == [\n",
    "        ('int', 'x', 3),\n",
    "        ('int', 'y', 5),\n",
    "        ('float', 'z', ('/', ('+', 'x', ('*', 2, 'y')), 3.0))\n",
    "    ]\n",
    "\n",
    "    # Test case 2\n",
    "    input_code = \"int a = 1 + 2 * 3 - 4 / 2;\"\n",
    "    tokens = lexer.tokenize(input_code)\n",
    "    ast = parser.parse(tokens)\n",
    "    assert ast == [\n",
    "        ('int', 'a', ('-', ('+', 1, ('*', 2, 3)), ('/', 4, 2)))\n",
    "    ]\n",
    "\n",
    "    # Test case 3 (Edge case)\n",
    "    input_code = \"int _x = (3);\"\n",
    "    tokens = lexer.tokenize(input_code)\n",
    "    ast = parser.parse(tokens)\n",
    "    assert ast == [\n",
    "        ('int', '_x', 3)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Run the test cases\n",
    "test_compiler()\n",
    "```\n",
    "\n",
    "In this code, we have defined two classes, `Lexer` and `Parser`, with their respective `tokenize` and `parse` methods left unimplemented. We have also provided three test cases that the students can use to check the correctness of their implementation. The test cases cover basic variable declaration and assignment, arithmetic operations, and edge cases with parentheses and variable names starting with an underscore."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
