{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98035648",
   "metadata": {},
   "source": [
    "# Introduction to Scraping Reddit with PRAW: A Friendly Guide for Humanities Researchers and Social Scientists\n",
    "\n",
    "Welcome, fellow researchers! In this tutorial, we'll be diving into the world of Reddit and learning how to gather valuable data from this popular platform using Python's PRAW package. Think of this tutorial as a friendly guide, walking you through the process and providing relatable metaphors to make it all a bit more accessible.\n",
    "\n",
    "## A Metaphor: The Library and the Librarian\n",
    "\n",
    "Before we begin, let's set the stage with a metaphor that might help make the concept of web scraping more relatable. Imagine you're standing in a vast library, with millions of books and resources at your disposal. However, finding the information you need is a daunting task without a useful index or a helpful librarian.\n",
    "\n",
    "In our case, Reddit is the library, and the PRAW package is our amiable librarian. Our goal is to learn how to communicate effectively with the librarian (PRAW) to get the information we need from the library (Reddit).\n",
    "\n",
    "## Chapter 1: Getting to Know the Librarian (PRAW)\n",
    "\n",
    "To start our journey, we first need to get to know our helpful librarian, PRAW. PRAW is short for Python Reddit API Wrapper. In simple terms, it's a tool that helps us interact with Reddit's data (the library) in an organized and efficient manner.\n",
    "\n",
    "### 1.1 Introducing Yourself\n",
    "\n",
    "Our first step is to introduce ourselves to the librarian. In the world of programming, this translates to installing the PRAW package and setting up our credentials to access Reddit's API (Application Programming Interface). The API is like a special access card to the library's resources.\n",
    "\n",
    "### 1.2 Learning the Librarian's Language\n",
    "\n",
    "Once we have introduced ourselves, we need to learn how to communicate effectively with our librarian. This means understanding the core concepts and terminology used in PRAW, such as:\n",
    "\n",
    "- Subreddits: These are like the different sections or departments within the library, housing books and resources on specific topics.\n",
    "- Posts: Each book in the library can be thought of as a post on Reddit. Posts have titles, authors, and content (text, images, or links).\n",
    "- Comments: Within each book (post), there are various discussions, opinions, and feedback from readers. These are similar to comments on Reddit.\n",
    "\n",
    "With these concepts in mind, we can now form our questions and requests in a way that our librarian, PRAW, can easily understand.\n",
    "\n",
    "## Chapter 2: Asking the Librarian for Help (Scraping Data)\n",
    "\n",
    "Now that we're familiar with PRAW and its language, it's time to ask the librarian for the information we need. This process involves specifying the subreddits, posts, and comments we're interested in and then gathering the data using PRAW.\n",
    "\n",
    "### 2.1 Browsing the Library's Sections (Subreddits)\n",
    "\n",
    "To begin, we'll tell PRAW which section of the library (subreddit) we want to explore. For example, if we're studying the impact of social media on mental health, we might ask the librarian to show us the books (posts) in the \"mental health\" section (subreddit).\n",
    "\n",
    "### 2.2 Selecting Books of Interest (Posts)\n",
    "\n",
    "Once we're in the right section, we need to decide which books (posts) we want to examine further. We can do this by specifying criteria such as the top-rated books, the most recent publications, or titles containing certain keywords.\n",
    "\n",
    "### 2.3 Delving into Reader Discussions (Comments)\n",
    "\n",
    "With our books in hand, we can now delve deeper into the reader discussions (comments) and gather insights from the conversations taking place. PRAW allows us to filter these comments based on criteria such as the most upvoted or the most controversial.\n",
    "\n",
    "## Chapter 3: Organizing and Analyzing Our Findings\n",
    "\n",
    "Finally, after collecting our data from the library (Reddit), we can organize and analyze it to uncover patterns, trends, and insights relevant to our research question. By using additional Python tools such as Pandas and Natural Language Processing libraries, we can turn our raw data into valuable knowledge, ready to be shared with our fellow researchers and social scientists.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "And there you have it! By understanding the metaphor of the library and the librarian, we can more easily grasp the concepts of web scraping with PRAW. With this new skill set, you'll be well-equipped to gather valuable data from Reddit and apply it to your various research projects in the humanities and social sciences. Happy scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34754559",
   "metadata": {},
   "source": [
    "# Scraping Reddit with Python PRAW Package for Humanities Researchers and Social Scientists\n",
    "\n",
    "As a humanities researcher or social scientist, you may often find yourself delving into the vast world of social media data to uncover insights and trends. One such platform is Reddit, which is a treasure trove of discussions, opinions, and valuable information. In this tutorial, we will learn how to scrape Reddit data using the Python PRAW package, designed specifically for this purpose.\n",
    "\n",
    "## Getting Started: Setting Up the Environment\n",
    "\n",
    "First, we need to install the PRAW package. You can do this by opening your terminal (or command prompt in Windows) and typing the following command:\n",
    "\n",
    "```\n",
    "pip install praw\n",
    "```\n",
    "\n",
    "Now that we have PRAW installed, let's start by importing the necessary libraries in our Jupyter Notebook:\n",
    "\n",
    "```python\n",
    "import praw\n",
    "```\n",
    "\n",
    "## Creating a Reddit App\n",
    "\n",
    "To access Reddit data, we will need to create a Reddit App that allows our Python script to authenticate and interact with the Reddit API. To do this, follow these steps:\n",
    "\n",
    "1. Go to https://www.reddit.com/prefs/apps\n",
    "2. Click on the \"Create App\" or \"Create Another App\" button at the bottom of the page.\n",
    "3. Fill in the form:\n",
    "   - **Name:** Choose a name for your app.\n",
    "   - **App type:** Select \"script\".\n",
    "   - **Description:** Write a brief description (optional).\n",
    "   - **About URL:** Leave it blank (optional).\n",
    "   - **Redirect URI:** Type \"http://localhost:8080\".\n",
    "   - **Choose a subreddit:** Leave it to \"Myself\".\n",
    "4. Click \"Create App\" at the bottom of the form.\n",
    "\n",
    "After successfully creating the app, you will see a \"Client ID\" and \"Client Secret\". Note these down, as we will use them in the next step.\n",
    "\n",
    "## Authenticating with Reddit API\n",
    "\n",
    "Now that we have our Reddit App credentials, let's set up our Python script to authenticate with the Reddit API:\n",
    "\n",
    "```python\n",
    "client_id = \"your_client_id\"  # Replace with your Client ID\n",
    "client_secret = \"your_client_secret\"  # Replace with your Client Secret\n",
    "user_agent = \"your_user_agent\"  # Replace with your preferred user agent name, e.g. \"MyRedditScraper\"\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
    "```\n",
    "\n",
    "## Scraping Reddit Data\n",
    "\n",
    "With our script authenticated, we can now start scraping Reddit data. In this example, we will scrape the top 10 posts from the \"worldnews\" subreddit:\n",
    "\n",
    "```python\n",
    "subreddit = reddit.subreddit(\"worldnews\")\n",
    "\n",
    "# Fetch the top 10 posts\n",
    "top_posts = subreddit.top(limit=10)\n",
    "\n",
    "# Iterate through the posts and print their titles and scores\n",
    "for post in top_posts:\n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"Score: {post.score}\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "This script fetches the top 10 posts from the \"worldnews\" subreddit and prints their titles and scores. You can modify the `limit` parameter to fetch more or fewer posts, and change the `subreddit` parameter to target a different subreddit.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have learned how to set up a Python environment to scrape Reddit data using the PRAW package. We covered the process of creating a Reddit App, authenticating with the Reddit API, and fetching data from a specific subreddit.\n",
    "\n",
    "As a humanities researcher or social scientist, you can apply these techniques to gather valuable insights from the Reddit platform. By analyzing the data, you can uncover trends, patterns, and opinions that will help you better understand the subjects you are studying.\n",
    "\n",
    "Happy scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826fa22a",
   "metadata": {},
   "source": [
    "Title: Analyzing Reddit's Top Posts in a Specific Category using PRAW\n",
    "\n",
    "Problem:\n",
    "\n",
    "As a humanities researcher or social scientist, you're interested in analyzing the top posts on Reddit within a specific category (also known as a subreddit) to gather insights and data for your research. Your task is to write a Python program that uses the PRAW package to scrape the top 10 posts of a given subreddit and prints the post's title, author, and score (upvotes - downvotes).\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Install the PRAW package by running the following command in your terminal or command prompt:\n",
    "\n",
    "```\n",
    "pip install praw\n",
    "```\n",
    "\n",
    "2. Set up a Reddit API application to obtain your client ID and secret by following the instructions here: https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example#first-steps\n",
    "\n",
    "3. Write a Python program that does the following:\n",
    "\n",
    "   a. Import the PRAW package and other necessary libraries.\n",
    "   \n",
    "   b. Use the obtained client ID, secret, and your Reddit account credentials to create a Reddit instance using PRAW.\n",
    "   \n",
    "   c. Define a function, `get_top_posts(subreddit_name, limit)`, that takes two parameters: `subreddit_name` (a string representing the name of the subreddit you want to analyze) and `limit` (an integer representing the number of top posts you want to retrieve). This function should use the PRAW package to access the specified subreddit and retrieve the top posts based on the provided limit.\n",
    "   \n",
    "   d. Inside the function, iterate through the top posts and print the title, author, and score for each post.\n",
    "   \n",
    "   e. Call the `get_top_posts` function with the desired subreddit and limit (e.g., \"worldnews\" and 10).\n",
    "\n",
    "Example Output:\n",
    "\n",
    "```\n",
    "Title: Example Post Title 1\n",
    "Author: example_author_1\n",
    "Score: 1000\n",
    "\n",
    "Title: Example Post Title 2\n",
    "Author: example_author_2\n",
    "Score: 2000\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "Note: The output will depend on the specified subreddit and the current top posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b57e8-0377-49f1-9a16-a43f9521b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "code correctly.\n",
    "\n",
    "Here is the code template with empty methods and comments:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import praw\n",
    "\n",
    "\n",
    "def create_reddit_instance(client_id, client_secret, user_agent, username, password):\n",
    "    # Create a Reddit instance using PRAW with the provided credentials\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_top_posts(reddit_instance, subreddit_name, limit):\n",
    "    # Access the specified subreddit using the Reddit instance\n",
    "    # Retrieve the top posts based on the provided limit\n",
    "    # Iterate through the top posts and print the title, author, and score for each post\n",
    "    pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Replace the placeholders with your Reddit API credentials\n",
    "    client_id = \"YOUR_CLIENT_ID\"\n",
    "    client_secret = \"YOUR_CLIENT_SECRET\"\n",
    "    user_agent = \"YOUR_USER_AGENT\"\n",
    "    username = \"YOUR_REDDIT_USERNAME\"\n",
    "    password = \"YOUR_REDDIT_PASSWORD\"\n",
    "\n",
    "    # Create a Reddit instance\n",
    "    reddit_instance = create_reddit_instance(client_id, client_secret, user_agent, username, password)\n",
    "\n",
    "    # Call the get_top_posts function with the desired subreddit and limit\n",
    "    get_top_posts(reddit_instance, \"worldnews\", 10)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "Now, let's create 3 assertion tests for the students to test their implementation:\n",
    "\n",
    "1. Test if the Reddit instance is created successfully.\n",
    "\n",
    "```python\n",
    "def test_create_reddit_instance():\n",
    "    reddit_instance = create_reddit_instance(\"YOUR_CLIENT_ID\", \"YOUR_CLIENT_SECRET\", \"YOUR_USER_AGENT\", \"YOUR_REDDIT_USERNAME\", \"YOUR_REDDIT_PASSWORD\")\n",
    "    assert isinstance(reddit_instance, praw.Reddit), \"Creating a Reddit instance failed.\"\n",
    "```\n",
    "\n",
    "2. Test if the specified subreddit is accessed correctly.\n",
    "\n",
    "```python\n",
    "def test_get_subreddit():\n",
    "    reddit_instance = create_reddit_instance(\"YOUR_CLIENT_ID\", \"YOUR_CLIENT_SECRET\", \"YOUR_USER_AGENT\", \"YOUR_REDDIT_USERNAME\", \"YOUR_REDDIT_PASSWORD\")\n",
    "    subreddit = get_subreddit(reddit_instance, \"worldnews\")\n",
    "    assert subreddit.display_name == \"worldnews\", \"Accessing the specified subreddit failed.\"\n",
    "```\n",
    "\n",
    "3. Test if the top posts are retrieved with the correct limit.\n",
    "\n",
    "```python\n",
    "def test_get_top_posts_limit():\n",
    "    reddit_instance = create_reddit_instance(\"YOUR_CLIENT_ID\", \"YOUR_CLIENT_SECRET\", \"YOUR_USER_AGENT\", \"YOUR_REDDIT_USERNAME\", \"YOUR_REDDIT_PASSWORD\")\n",
    "    top_posts = get_top_posts(reddit_instance, \"worldnews\", 5)\n",
    "    assert len(top_posts) == 5, \"Retrieving top posts with the correct limit failed.\"\n",
    "```\n",
    "\n",
    "Students should implement the missing functionality in the provided code template and then use these tests to verify if their implementation is correct. The tests should be run using a testing framework like `pytest`. Make sure to replace the placeholders with your Reddit API credentials before running the tests."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
